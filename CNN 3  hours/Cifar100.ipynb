{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverCore97/CNN-exercises/blob/main/CNN%203%20%20hours/Cifar100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_PtgxJgBpfW",
        "outputId": "02f8d12d-95a1-4635-ef68-4e587ea9318e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 loss: 4.6053466796875\n",
            "0 100 loss: 4.599911212921143\n",
            "0 200 loss: 4.433727741241455\n",
            "0 300 loss: 4.238964557647705\n",
            "0 400 loss: 4.388631820678711\n",
            "0 500 loss: 4.0995893478393555\n",
            "0 600 loss: 3.993873357772827\n",
            "0 700 loss: 3.942411422729492\n",
            "0 acc 0.0587\n",
            "1 0 loss: 4.1155290603637695\n",
            "1 100 loss: 4.218831539154053\n",
            "1 200 loss: 3.7031092643737793\n",
            "1 300 loss: 3.967514991760254\n",
            "1 400 loss: 3.6305854320526123\n",
            "1 500 loss: 3.9125266075134277\n",
            "1 600 loss: 3.6464195251464844\n",
            "1 700 loss: 3.81416654586792\n",
            "1 acc 0.1156\n",
            "2 0 loss: 3.84089994430542\n",
            "2 100 loss: 3.456129550933838\n",
            "2 200 loss: 3.6624484062194824\n",
            "2 300 loss: 3.8472816944122314\n",
            "2 400 loss: 3.279440402984619\n",
            "2 500 loss: 3.543823719024658\n",
            "2 600 loss: 3.844877243041992\n",
            "2 700 loss: 3.525057792663574\n",
            "2 acc 0.1726\n",
            "3 0 loss: 3.4385766983032227\n",
            "3 100 loss: 3.2946395874023438\n",
            "3 200 loss: 3.4086861610412598\n",
            "3 300 loss: 3.3527066707611084\n",
            "3 400 loss: 3.7126102447509766\n",
            "3 500 loss: 3.338181972503662\n",
            "3 600 loss: 3.2416892051696777\n",
            "3 700 loss: 3.315552234649658\n",
            "3 acc 0.2091\n",
            "4 0 loss: 3.6755831241607666\n",
            "4 100 loss: 3.3886489868164062\n",
            "4 200 loss: 3.3203859329223633\n",
            "4 300 loss: 3.2043118476867676\n",
            "4 400 loss: 3.0397002696990967\n",
            "4 500 loss: 3.487912178039551\n",
            "4 600 loss: 2.8512682914733887\n",
            "4 700 loss: 2.8145601749420166\n",
            "4 acc 0.2538\n",
            "5 0 loss: 3.0517404079437256\n",
            "5 100 loss: 3.1546058654785156\n",
            "5 200 loss: 2.637610673904419\n",
            "5 300 loss: 2.89068603515625\n",
            "5 400 loss: 2.787257194519043\n",
            "5 500 loss: 2.8356242179870605\n",
            "5 600 loss: 3.127169609069824\n",
            "5 700 loss: 2.859095811843872\n",
            "5 acc 0.2663\n",
            "6 0 loss: 3.040236711502075\n",
            "6 100 loss: 2.594010353088379\n",
            "6 200 loss: 2.355281352996826\n",
            "6 300 loss: 2.610196113586426\n",
            "6 400 loss: 2.997206211090088\n",
            "6 500 loss: 2.9608986377716064\n",
            "6 600 loss: 3.1050777435302734\n",
            "6 700 loss: 2.412520408630371\n",
            "6 acc 0.298\n",
            "7 0 loss: 2.4769372940063477\n",
            "7 100 loss: 2.7977733612060547\n",
            "7 200 loss: 2.803647518157959\n",
            "7 300 loss: 2.5749881267547607\n",
            "7 400 loss: 2.6001741886138916\n",
            "7 500 loss: 2.105605125427246\n",
            "7 600 loss: 2.5346498489379883\n",
            "7 700 loss: 2.3809847831726074\n",
            "7 acc 0.3229\n",
            "8 0 loss: 2.1453444957733154\n",
            "8 100 loss: 2.805586814880371\n",
            "8 200 loss: 2.1898157596588135\n",
            "8 300 loss: 2.600642204284668\n",
            "8 400 loss: 2.0010480880737305\n",
            "8 500 loss: 2.694080352783203\n",
            "8 600 loss: 2.2515366077423096\n",
            "8 700 loss: 2.210012912750244\n",
            "8 acc 0.3339\n",
            "9 0 loss: 2.4954276084899902\n",
            "9 100 loss: 2.198943614959717\n",
            "9 200 loss: 2.4347500801086426\n",
            "9 300 loss: 2.122145652770996\n",
            "9 400 loss: 2.183612823486328\n",
            "9 500 loss: 2.728297472000122\n",
            "9 600 loss: 2.572725534439087\n",
            "9 700 loss: 1.893038272857666\n",
            "9 acc 0.3474\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers,Sequential,optimizers,datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(2345)\n",
        "conv_layers=[\n",
        "            #unit 1\n",
        "           layers.Conv2D(64,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.Conv2D(64,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.MaxPool2D(strides=2,padding='same'),\n",
        "\n",
        "           #unit 2\n",
        "           layers.Conv2D(128,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.Conv2D(128,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.MaxPool2D(strides=2,padding='same'),\n",
        "\n",
        "           #unit 3\n",
        "           layers.Conv2D(256,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.Conv2D(256,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.MaxPool2D(strides=2,padding='same'),\n",
        "\n",
        "           #unit 4\n",
        "           layers.Conv2D(512,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.Conv2D(512,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.MaxPool2D(strides=2,padding='same'),\n",
        "\n",
        "           #unit 5\n",
        "           layers.Conv2D(512,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.Conv2D(512,3,padding='same',activation=tf.nn.relu),\n",
        "           layers.MaxPool2D(strides=2,padding='same')\n",
        "]\n",
        "\n",
        "#preprocess x,y x to 0-1\n",
        "#set x to float and divide by 255 - Normalization\n",
        "#squeeze out the dimension from y and set to int\n",
        "def preprocess(x,y):\n",
        "  x=tf.cast(x,dtype=tf.float32)/255.\n",
        "  y=tf.cast(y,dtype=tf.int32)\n",
        "  return x,y\n",
        "\n",
        "conv_net=Sequential(conv_layers)\n",
        "\n",
        "fc_layers=[\n",
        "           layers.Dense(256,activation=tf.nn.relu),\n",
        "           layers.Dense(128,activation=tf.nn.relu),\n",
        "           layers.Dense(100,activation=None)\n",
        "           ]\n",
        "fc_net=Sequential(fc_layers)\n",
        "\n",
        "conv_net.build(input_shape=[None,32,32,3])\n",
        "\n",
        "fc_net.build(input_shape=[None,512])\n",
        "\n",
        "optimizer=optimizers.Adam(lr=1e-4)\n",
        "# Get dataset\n",
        "(x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
        "y=tf.squeeze(y,axis=1)\n",
        "y_test=tf.squeeze(y_test,axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#enter x,y in training function using the layers\n",
        "train_db=tf.data.Dataset.from_tensor_slices((x,y))\n",
        "train_db= train_db.shuffle(1000).map(preprocess).batch(64)\n",
        "\n",
        "test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
        "test_db= test_db.map(preprocess).batch(64)\n",
        "\n",
        "sample=next(iter(train_db))\n",
        "\n",
        "variables=conv_net.trainable_variables+fc_net.trainable_variables\n",
        "#run the training function for multiple epochs\n",
        "for epoch in range(10):\n",
        "  for step, (x,y) in enumerate(train_db):  \n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      out=conv_net(x)\n",
        "      out =tf.reshape(out,[-1,512])\n",
        "      logits=fc_net(out)\n",
        "      #use 1-hot-encoding on y\n",
        "      y_onehot=tf.one_hot(y,depth=100)\n",
        "      #Calculate losses with crossentropy\n",
        "      loss=tf.losses.categorical_crossentropy(y_onehot,logits,from_logits=True)\n",
        "      loss=tf.reduce_mean(loss)\n",
        "\n",
        "      \n",
        "    #Use adam optimizer to apply gradients to the weights\n",
        "    grads=tape.gradient(loss,variables)\n",
        "    optimizer.apply_gradients(zip(grads,variables))\n",
        "\n",
        "    if step%100==0:\n",
        "      print(epoch,step,\"loss:\", float(loss))\n",
        "  \n",
        "  #test\n",
        "  total_number=0\n",
        "  total_correct=0\n",
        "  for x,y in test_db:\n",
        "    out=conv_net(x)\n",
        "    out = tf.reshape(out,[-1,512])\n",
        "    logits=fc_net(out)\n",
        "    #Calculate probabilities for each of the 100 categories\n",
        "    prob=tf.nn.softmax(logits,axis=1)\n",
        "    # Choose the category with the maximum value\n",
        "    pred=tf.argmax(prob,axis=1)\n",
        "    # Recast the answer into tf.int32\n",
        "    pred=tf.cast(pred,dtype= tf.int32)\n",
        "\n",
        "    correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n",
        "    correct=tf.reduce_sum(correct)\n",
        "\n",
        "    total_correct+=int(correct)\n",
        "    total_number+=x.shape[0]\n",
        "\n",
        "  accuracy= total_correct/total_number\n",
        "\n",
        "  print(epoch, \"acc\", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zVrClGfUhMF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cifar100.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOi/YfQvzop7cXqKWxCCNxy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}